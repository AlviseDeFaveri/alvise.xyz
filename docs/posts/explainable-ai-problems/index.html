<!DOCTYPE html>
<html lang="en-us" data-anim="none">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Explainable AI: What the Hell is Even That? - alvise.xyz </title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="/main.min.5047b4e57e44e942a748ba9c0dee5b480749624017c5bcfb7d0ce89251eed0cc.css" media="all">

    
    <script type="text/javascript">
      var storedTheme = localStorage.getItem('theme') || (window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light");
      if (storedTheme) {
        document.documentElement.setAttribute('data-theme', storedTheme)
      } else {
        document.documentElement.setAttribute('data-theme', 'dark')
      }
    </script>

    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="Explainable AI: What the Hell is Even That?" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.alvise.xyz/posts/explainable-ai-problems/" />


  
<meta property="og:image" content="https://www.alvise.xyz/posts/explainable-ai-problems/img/ml_hu711e1ee2b9b6b0bd24ce1182739eb66b_41184_400x0_resize_q75_box.jpg">

<meta property="article:published_time" content="2022-08-10T00:00:00+00:00" />





<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://www.alvise.xyz/posts/explainable-ai-problems/img/ml_hu711e1ee2b9b6b0bd24ce1182739eb66b_41184_400x0_resize_q75_box.jpg">



<meta name="twitter:title" content="Explainable AI: What the Hell is Even That?"/>
<meta name="twitter:description" content="Machine Learning Memes for Convolutional Teens
Some time ago I wrote a (non-technical) short essay on AI explanation for a uni course. I didn&rsquo;t know anything about the topic at the time (and things haven&rsquo;t changed much since then) but it already seemed quite clear that the problem of explanation would become a huge one as AI research advanced."/>

</head>
<body class="terminal">
      <div class="container top-container">
        <div class="terminal-nav">
          <header class="terminal-logo">

            
          </header>
          
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
              
              
              <a class=home-btn href="https://www.alvise.xyz/">~/</a>
              
              <li class="separator"></li>
              <li><button id="theme-toggle">üåìÔ∏é</button></li>
            </ul>
          </nav>
        </div>
    </div>

    <div class="container main-container" >
        
<h1>Explainable AI: What the Hell is Even That?</h1>

Aug. 10, 2022


<br/><br/>
<figure
style="max-width:600px;
"><img src="img/ml.jpg.webp"
         alt="Machine Learning Memes for Convolutional Teens"style=";;;
         "
    /><figcaption>
            <p>Machine Learning Memes for Convolutional Teens</p>
        </figcaption>
</figure>

<p>Some time ago I wrote a (non-technical) <a href="https://raw.githubusercontent.com/AlviseDeFaveri/XAI-paper/master/main.pdf">short
essay</a>
on AI explanation for a uni course. I didn&rsquo;t know anything about the topic at
the time (and things haven&rsquo;t changed much since then) but it already seemed quite clear
that the problem of explanation would become a huge one as AI research advanced.</p>
<!-- raw HTML omitted -->
<p>There were already many ideas on how to do it, from using simpler algorithms
like decision trees to approximate a neural network to producing AIs that can
learn how to explain themselves.</p>
<p>However, some of the limitations were also already clear.
For example: how do you measure the quality of an explanation? Is it about how
<em>faithful</em> they are (but then, the most faithful explanation is the code itself)
or how <em>convincing</em> they are? If this is the case, isn&rsquo;t that a clear
<em>incentive to lie</em>? This is basically what my essay is about.</p>
<p>Clearly, the reasoning is very abstract and only scratches the surface, but I think the
core of the question is still interesting and worthy to investigate.</p>
<hr>
<p>Fast-forward in time: it&rsquo;s now 2023, ChatGPT and LLMs are huge on Twitter (maybe
less so in the real world) and obviously the research on explainable AI is now
hotter than ever. Also, Twitter is now X, but that&rsquo;s another story.</p>
<p>Here&rsquo;s a (very random and outdated) collection of interesting articles about the
topic:</p>
<ul>
<li><a href="https://medium.com/dsaid-govtech/towards-a-comparable-metric-for-ai-model-interpretability-part-1-d55d4bae8a58">This article</a> from 2022, written by researchers from Meta and from the Singaporean government (for some reason), talks exactly about the absence of a common metric for measuring interpretability in AI models.
They also claim a completely negative correlation between an AI model&rsquo;s accuracy and it&rsquo;s out-of-the-box explainability. Sounds like we&rsquo;re going to have a lot of fun with opaque, broken AI for the forseeable future!</li>
</ul>
<figure
style="max-width:700px;
"><img src="img/interpretability-vs-accuracy.webp"
         alt="Interpretability vs Accuracy of modern AI models"style=";;;
         "
    /><figcaption>
            <p>Interpretability vs Accuracy of modern AI models</p>
        </figcaption>
</figure>

<ul>
<li><a href="https://arxiv.org/abs/2304.03279">This paper</a> talks about how deception can be a reward for different kinds of LLMs by creating a benchmark of 134 choose-your-own-adventure game where the AI had to play, and then measuring which strategy it decided to adopt. Very fascinating, indeed.</li>
</ul>
<figure
style="max-width:700px;
"><img src="img/machiavelli.png"
         alt="The Machiavelli benchmark"style=";;;
         "
    /><figcaption>
            <p>The Machiavelli benchmark</p>
        </figcaption>
</figure>

<ul>
<li><a href="https://arxiv.org/abs/2308.02312">This paper</a> shows that users prefer ChatGPT&rsquo;s answers due to language style even when they are inaccurate or verbose. Indeed, we do have a framing bias, and it shows.</li>
</ul>
<div class="twitter" style="height: 650px;">

    <blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">Is there a name for this? ‚ÄúOur examination revealed that 52% of ChatGPT&#39;s answers contain inaccuracies and 77% are verbose. Nevertheless, users still prefer ChatGPT&#39;s responses 39% of the time due to their comprehensiveness and articulate language style.‚Äù <a href="https://t.co/uivK8xjFnL">https://t.co/uivK8xjFnL</a></p>&mdash; Gary Marcus (@GaryMarcus) <a href="https://twitter.com/GaryMarcus/status/1689851180657397760?ref_src=twsrc%5Etfw">August 11, 2023</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


  </div>




        <footer class="footer">
  <p>Built with
    <a href="https://gohugo.io/">Hugo</a> and
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console</a>
  </p>
</footer>

    </div>
  </body>



<script type="text/javascript" src=/js/repo-card.js></script>



<script type="text/javascript">
  var toggle = document.getElementById("theme-toggle");

  toggle.onclick = function() {
    document.documentElement.setAttribute("data-anim", "fast")

    var currentTheme = document.documentElement.getAttribute("data-theme");
    var targetTheme = currentTheme === "light" ? "dark" : "light";

    document.documentElement.setAttribute("data-theme", targetTheme)
    localStorage.setItem("theme", targetTheme);
  };
</script>
</html>
