---
Title: "Will AI explanation save us from the bias problem?"
date: 2022-07-07
RepoCard: true
math: false
highlight: false
image: img/ml.jpg
summary: During my last year of university I wrote a short paper on AI explanation as part of the final examination for the Phylosophical Issues in Computer Science course. Although this is definitely not my main field of expertise, I found that reading and reflecting on the problem of giving people a right for an explanation and trying to understand and eliminate the bias that we...
---

{{< figure width=800px src=img/ml.jpg.webp caption="Machine Learning Memes for Convolutional Teens">}}

During my last year of university I wrote a short paper on AI explanation as part of the final examination for the _Phylosophical Issues in Computer Science_ course.

Although this is definitely not my main field of expertise, I found that reading and reflecting on the problem of giving people a right for an explanation and trying to understand and eliminate the bias that we, as humans, introduce in AI systems is a very fascinating topic, and probably something that has no clear solution.

In this short paper I introduce some of the main concepts of XAI (eXplainable AI) that I found in the current literature, and I present a small thought experiment to make the case that explanation interfaces can't per-se eliminate bias, and they can actually make the problem worse by framing the explanation in the right way.

So, if youâ€™re interested in a reflection on the opportunities (and possible risks) of AI explanation, [here](https://raw.githubusercontent.com/AlviseDeFaveri/XAI-paper/master/main.pdf) you go. I'm not an expert, so consider yourself warned.

{{< github "AlviseDeFaveri/XAI-paper" >}}

Have a good one!

~A
